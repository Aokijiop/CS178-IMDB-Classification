{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f76e7e-2303-4a15-bc6f-dbee6f7a8d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (2.19.2)\n",
      "Requirement already satisfied: click in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.1 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (0.23.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from requests>=2.32.1->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from requests>=2.32.1->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from requests>=2.32.1->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aaoki\\miniconda3\\envs\\cs178\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the needed libraries\n",
    "!pip install nltk gensim datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b20a47e-2a22-408e-bde2-578bf5df2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185f8754-69d8-41fc-b172-c7ef0589b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e85b087-8141-421f-9748-b152f0bbfe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (text) and labels from the training portion of the dataset\n",
    "X_tr = dataset['train']['text']\n",
    "y_tr = dataset['train']['label']\n",
    "\n",
    "# Extract features (text) and labels from the testing portion of the dataset\n",
    "X_te = dataset['test']['text']\n",
    "y_te = dataset['test']['label']\n",
    "\n",
    "# Tokenize the text data (Flag: Might want consider using a tokenizer for this later)\n",
    "X_tr_tokenized = [text.split() for text in X_tr]\n",
    "X_te_tokenized = [text.split() for text in X_te]\n",
    "\n",
    "# Train Word2Vec model on the tokenized text data\n",
    "\n",
    "# Hyperparameters:\n",
    "# vector_size: Size of the vector produced for a given word\n",
    "# window: The maximum distance between the current and predicted word within a sentence. Words outside this window are not considered in the context.\n",
    "# min_count: The minimum number of times a word must appear in the sentence to be considered\n",
    "\n",
    "# Other parameters:\n",
    "# workers: The amount of threads to be used\n",
    "word2vec_model = Word2Vec(sentences=X_tr_tokenized, vector_size=300, window=100, min_count=1, workers=4)\n",
    "\n",
    "# Function to convert text to Word2Vec embeddings\n",
    "def get_word2vec_embeddings(text_data, model):\n",
    "    embeddings = []\n",
    "    for text in text_data:\n",
    "        wordvecs = [model.wv[word] for word in text if word in model.wv]\n",
    "        if wordvecs:\n",
    "            embeddings.append(np.mean(wordvecs, axis=0))\n",
    "        else:\n",
    "            embeddings.append(np.zeros(model.vector_size))\n",
    "    return embeddings\n",
    "\n",
    "# Convert the text data to Word2Vec embeddings\n",
    "X_tr_vec = get_word2vec_embeddings(X_tr_tokenized, word2vec_model)\n",
    "X_te_vec = get_word2vec_embeddings(X_te_tokenized, word2vec_model)\n",
    "\n",
    "# Standardize the vector length\n",
    "scaler = StandardScaler()\n",
    "X_tr_vec = scaler.fit_transform(X_tr_vec)\n",
    "X_te_vec = scaler.transform(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21604a8f-fcfb-4b5c-af6e-13df001364bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74     12500\n",
      "           1       0.77      0.61      0.68     12500\n",
      "\n",
      "    accuracy                           0.71     25000\n",
      "   macro avg       0.72      0.71      0.71     25000\n",
      "weighted avg       0.72      0.71      0.71     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train kNN classifier\n",
    "\n",
    "# Hyperparameters:\n",
    "# n_neigbors: The number of neighbors to consider\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=300)\n",
    "knn_classifier.fit(X_tr_vec, y_tr)\n",
    "\n",
    "# Evaluate the classifier on the testing set\n",
    "y_pred = knn_classifier.predict(X_te_vec)\n",
    "print(classification_report(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1086c-d5fe-4703-9fd9-d5e6d0205732",
   "metadata": {},
   "source": [
    "### Log of hyperparameters and associated reports (View markdown for better formatting):\n",
    "\n",
    "vector_size=100, window=5, min_count=1, workers=4, n_neighbors=5\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.65      0.70      0.67     12500\n",
    "           1       0.67      0.62      0.64     12500\n",
    "\n",
    "    accuracy                           0.66     25000\n",
    "    macro avg       0.66      0.66      0.66     25000\n",
    "    weighted avg       0.66      0.66      0.66     25000\n",
    "\n",
    "<hr>\n",
    "vector_size=300, window=5, min_count=1, workers=4, n_neighbors=5\n",
    "\n",
    "              precision    recall  f1-score   support\r\n",
    "\r\n",
    "           0       0.65      0.69      0.67     12500\r\n",
    "           1       0.67      0.63      0.65     12500\r\n",
    "\r\n",
    "    accuracy                           0.66     \n",
    "    0\r\n",
    "   mvg             0.66      0.66      0.66    \n",
    "    2500hted avg              0.66      0.66      0.66   \n",
    "\n",
    "<hr>\n",
    "vector_size=300, window=5, min_count=1, workers=4, n_neighbors=10\n",
    "\n",
    "             precision    recall  f1-score   support\r\n",
    "\r\n",
    "           0       0.64      0.78      0.70     12500\r\n",
    "           1       0.72      0.55      0.62     12500\r\n",
    "\r\n",
    "    accuracy                           0.67     \n",
    "    0\r\n",
    "   macro avg       0.68      0.67      0.66    \n",
    "    25000\r\n",
    "weighted avg       0.68      0.67      0.66   \n",
    "\n",
    "<hr>\n",
    "vector_size=300, window=10, min_count=1, workers=4, n_neighbors=10\n",
    "\n",
    "              precision    recall  f1-score   support\r\n",
    "\r\n",
    "           0       0.65      0.78      0.71     12500\r\n",
    "           1       0.73      0.58      0.64     12500\r\n",
    "\r\n",
    "    accuracy                           0.68     \n",
    "    0\r\n",
    "   macro avg       0.69      0.68      0.68    \n",
    "    25000\r\n",
    "weighted avg       0.69      0.68      0.68   \n",
    "\n",
    "<hr>\n",
    "vector_size=300, window=10, min_count=1, workers=4, n_neighbors=300\n",
    "\n",
    "             precision    recall  f1-score   support\r\n",
    "\r\n",
    "           0       0.65      0.80      0.72     12500\r\n",
    "           1       0.74      0.57      0.64     12500\r\n",
    "\r\n",
    "    accuracy                           0.68     \n",
    "    0\r\n",
    "   macro avg       0.69      0.68      0.68    \n",
    "    25000\r\n",
    "weighted avg       0.69      0.68      0.68   \n",
    "\n",
    "<hr>\n",
    "vector_size=300, window=100, min_count=1, workers=4, n_neighbors=300\n",
    "\n",
    "              precision    recall  f1-score   support\r\n",
    "\r\n",
    "           0       0.68      0.81      0.74     12500\r\n",
    "           1       0.77      0.61      0.68     12500\r\n",
    "\r\n",
    "    accuracy                           0.71     2500 0\r\n",
    "   macro avg       0.72      0.71      0.71         25000\r\n",
    "weighted avg       0.72      0.71      0.71     25000  25000  25000  25000  25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e0ea52-4a46-4b5d-bacc-33f21fa6de78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform PCA for dimensionality reduction\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mPCA\u001b[49m(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m X_train_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(X_train_wordvecs)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Plot the data points\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_wordvecs)\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', alpha=0.5)\n",
    "plt.title('Word Embeddings (Word2Vec) - PCA Visualization')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a24447-3589-4737-95fc-fce4e6d367ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
